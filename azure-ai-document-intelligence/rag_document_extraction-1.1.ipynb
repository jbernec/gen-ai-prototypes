{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a595da52-a4ed-4c85-bbea-f6a1a03cedda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Developed using API version 2024-02-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "258117ed-f569-4318-8f94-30f1f7b64ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) with Azure AI Document Intelligence.\n",
    "\n",
    "In an earlier notebook, I demonstrated how Azure AI Search can automatically convert data into vectors using the built-in vectorization feature. It can manage the entire workflow of pipeline tasks from ingestion, extraction, enrichment and data upload to the search index with minimal or no custom coding. However, a drawback is that the existing skills may not capture all the relevant content from the document.\n",
    "\n",
    "In this notebook, I demonstrate a solution that uses the prebuilt layout model of the Azure AI Document Intelligence resource to get all the necessary content from the PDF booklet and enable the semantic chunking feature. This should overcome the encountered limitation with the previous solution and improve the relevance and accuracy of the search retrieval.\n",
    "\n",
    "This is the first of two notebooks, which shows a solution that uses Azure AI Document Intelligence and Langchain to create a Retrieval Augmented Generation (RAG) workflow. It uses the Langchain Azure AI Document Intelligence document loader to get tables, paragraphs, and layout information from a PDF file. The output is in markdown format, which is processed by Langchain's markdown header splitter. This allows the semantic chunking feature of Azure AI Document Intelligence service to produce semantic chunks of the source document \n",
    "\n",
    "We employ the AI Search Python SDK to build the Azure AI Search index, load the semantically chunked documents into this index and execute a hybrid + semantic search query at the end of the notebook to assess the search result relevance.\n",
    "\n",
    "![Semantic chunking in RAG](https://github.com/jbernec/rag-orchestrations/blob/main/images/semantic-chunking.png?raw=true)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- An Azure AI Document Intelligence resource - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don't have.\n",
    "- An Azure AI Search resource - follow [this document](https://learn.microsoft.com/azure/search/search-create-service-portal) to create one if you don't have.\n",
    "- An Azure OpenAI resource and deployments for embeddings model and chat model - follow [this document](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal) to create one if you don't have.\n",
    "- I have attached a requirements file in the repo folder as this notebook to show the python libraries required for this poc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb63d4db-bfa7-48b5-8483-f5530ac96ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, AnalyzeDocumentRequest, ContentFormat\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import base64\n",
    "from openai import AzureOpenAI\n",
    "import azure.identity\n",
    "from azure.identity import DefaultAzureCredential, EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential\n",
    "from azure.identity import ClientSecretCredential\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "650185d0-e0dd-413f-9871-9945e36068b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads and sets the necessary variables for Azure services.\n",
    "The variables are loaded from Azure Key Vault.\n",
    "\"\"\"\n",
    "azure_openai_endpoint=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-endpoint\")\n",
    "azure_openai_api_key=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-api-key\")\n",
    "azure_openai_api_version = \"2024-02-15-preview\"\n",
    "azure_openai_embedding_deployment = dbutils.secrets.get(scope=\"myscope\", key=\"aoai-embedding-deployment\")\n",
    "azure_openai_embedding_model = dbutils.secrets.get(scope=\"myscope\", key=\"aoai-embedding-model\")\n",
    "doc_intelligence_endpoint = dbutils.secrets.get(scope=\"myscope\", key=\"docintelligence-endpoint\")\n",
    "doc_intelligence_key = dbutils.secrets.get(scope=\"myscope\", key=\"docintelligence-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9880fb-f9de-4919-bbfe-2287baed6ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Blob Storage\n",
    "# blob_connection_string = dbutils.secrets.get(scope=\"myscope\", key=\"blobstore-connstr\")\n",
    "# blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "\n",
    "# Service principal authentication variables\n",
    "tenant_id=dbutils.secrets.get(scope=\"myscope\", key=\"tenantid\")\n",
    "client_id = dbutils.secrets.get(scope=\"myscope\", key=\"clientid\")\n",
    "client_secret = dbutils.secrets.get(scope=\"myscope\", key=\"clientsecret\")\n",
    "credential = azure.identity.ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n",
    "\n",
    "blob_storage_name = \"blobstore05\" #dbutils.secrets.get(scope=\"myscope\", key=\"blobstore-account-name\")\n",
    "# Use the above defined service principal to authenticate against the blob storage endpoint of the ADLS Gen 2 service\n",
    "blob_service_client = BlobServiceClient(\n",
    "    account_url=f\"https://{blob_storage_name}.blob.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "blob_container_name = \"document-intelligence\"\n",
    "container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "container_url = container_client.url\n",
    "blobs = container_client.list_blobs()\n",
    "first_blob = blobs.next()\n",
    "blob_url = container_client.get_blob_client(first_blob).url\n",
    "print(f\"URL of first blob: {blob_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c81c08db-97d7-4d96-826e-d07674077c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utility Function Definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d481dc4-2600-44ef-b838-56cb75f40a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_deployment=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b77de70-fdd8-48ec-9a78-b7abf51e98a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_pdf(url: str):\n",
    "    print(f\"{url}\\n\\n\")\n",
    "    print(f\"---------------------------------------------\")\n",
    "    \n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=doc_intelligence_endpoint, credential=AzureKeyCredential(key=doc_intelligence_key), api_version=\"2024-02-29-preview\")\n",
    "\n",
    "    poller= document_intelligence_client.begin_analyze_document(model_id=\"prebuilt-layout\", analyze_request=AnalyzeDocumentRequest(url_source=url), output_content_format=\"markdown\")\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_table_description(table_content):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following table and its context from the original document,\n",
    "    provide a detailed description of the table. Then, include the table in markdown format.\n",
    "\n",
    "    Table Content:\n",
    "    {table_content}\n",
    "\n",
    "    Please provide:\n",
    "    1. A comprehensive description of the table.\n",
    "    2. The table in markdown format.\n",
    "    \"\"\"\n",
    "\n",
    "    response = azure_openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes tables and formats them in markdown.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def capture_table_info(result: AnalyzeResult):\n",
    "    # Initialize an empty dictionary to store table information\n",
    "    table_info = {}\n",
    "\n",
    "    if result.tables:\n",
    "        for table_idx, table in enumerate(result.tables):\n",
    "            # Initialize a dictionary to store information about each table\n",
    "            table_details = {\n",
    "                \"page_number\": None,\n",
    "                \"content\": \"\"\n",
    "            }\n",
    "\n",
    "            # Capture table location information\n",
    "            if table.bounding_regions:\n",
    "                table_details[\"page_number\"] = int(table.bounding_regions[0].page_number)\n",
    "\n",
    "            # Capture raw table content\n",
    "            for page in result.pages:\n",
    "                if page.page_number == table_details[\"page_number\"]:\n",
    "                    start_pos = page.spans[0].offset\n",
    "                    end_pos = start_pos + page.spans[0].length\n",
    "                    table_details[\"content\"] += result.content[start_pos:end_pos] + \" \"\n",
    "\n",
    "            # Store the table details in the dictionary using the table index as the key\n",
    "            table_info[f\"Table_{table_idx}\"] = table_details\n",
    "\n",
    "    return table_info\n",
    "\n",
    "# Example usage:\n",
    "# table_info = capture_table_info(result)\n",
    "# Now, table_info dictionary contains all the details about the tables\n",
    "# You can retrieve the information about any table using its index, for example, table_info[\"Table_0\"]\n",
    "    \n",
    "# Function to crack and extract PDF documents using Azure AI Document Intelligence\n",
    "def parse_pdf_content(result: AnalyzeResult):\n",
    "    # Initialize an empty list to store page information\n",
    "    page_list = []\n",
    "\n",
    "    for page in result.pages:\n",
    "        page_num = page.page_number\n",
    "        page_content = \"\"\n",
    "\n",
    "        # Capture raw page content\n",
    "        start_pos = page.spans[0].offset\n",
    "        end_pos = start_pos + page.spans[0].length\n",
    "        page_content += result.content[start_pos:end_pos] + \" \"\n",
    "\n",
    "        # Check if there are tables on the current page\n",
    "        tables_on_page = [table for table in result.tables if table.bounding_regions[0].page_number == page_num]\n",
    "        if tables_on_page:\n",
    "            # If there are tables on the page, label the page as type: \"table\"\n",
    "            page_type = \"table\"\n",
    "        else:\n",
    "            page_type = \"page\"\n",
    "\n",
    "        # Add the page information to the list\n",
    "        page_list.append({\n",
    "            \"type\": page_type,\n",
    "            \"page_number\": page_num,\n",
    "            \"content\": page_content\n",
    "        })\n",
    "\n",
    "    return page_list\n",
    "\n",
    "# Example usage:\n",
    "# result = extract_pdf(url=blob_url)\n",
    "# page_list = extract_and_capture_info(result)\n",
    "# Now, page_list contains all the pages with their type labeled accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "235eba57-16fb-410d-854f-495c43946236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load a document and split it into semantic chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d6b9f2f-9423-43db-b1c3-6d2c1d5a95fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create a new index with custom filterable and retrievable fields and upload the data to the content field of the search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c93c6a5-2752-419f-a1e0-44b82c7df6ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the search index fields and vector search configuration\n",
    "\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration, SemanticSearch, SemanticPrioritizedFields, SemanticField\n",
    ")\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "import os\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.identity import DefaultAzureCredential, AzureAuthorityHosts\n",
    "\n",
    "\n",
    "fields = [\n",
    "    SearchField(name=\"parent_id\",key=True,type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"title\",type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"chunk\",type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"location\",type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"pagenum\",type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\",type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "# Define the vector search configuration and parameters\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(name=\"myHsnw\")\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHsnw\",\n",
    "            vectorizer_name=\"myOpenAI\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myOpenAI\",\n",
    "            kind=\"azureOpenAI\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_deployment,\n",
    "                model_name=azure_openai_embedding_model,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure semantic search on the index\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic search config\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "scoring_profiles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022453df-e8b9-45a6-99e6-6909d41feb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a search index client required to create the index\n",
    "search_credential = AzureKeyCredential(dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-key\"))\n",
    "search_endpoint = dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-endpoint\")\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "\n",
    "index_name = \"benefits-index\"\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, scoring_profiles=scoring_profiles, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index=index)\n",
    "print(f\"{result.name} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a5a1b5-936e-44c9-9c2c-a62c106089a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Upload documents to AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50432508-a888-4ffd-a03a-a7da4ad1dc33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the langchain azure open ai embedding object. This will be used to embed the vector field content\n",
    "# https://python.langchain.com/v0.1/docs/integrations/vectorstores/azuresearch/#create-embeddings-and-vector-store-instances\n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2886fd30-4734-4fdc-9e8e-df31c72894c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def text_to_base64(text):\n",
    "    # Convert text to bytes using UTF-8 encoding\n",
    "    # and use this function for generating a unique value for the Azure AI Search Index parent_id values\n",
    "    bytes_data = text.encode('utf-8')\n",
    "\n",
    "    # Perform Base64 encoding\n",
    "    base64_encoded = base64.b64encode(bytes_data)\n",
    "\n",
    "    # Convert the result back to a UTF-8 string representation\n",
    "    base64_text = base64_encoded.decode('utf-8')\n",
    "\n",
    "    return base64_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab8706e-2109-4fc8-b3f0-a87f9cbba849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Upload the semantically chunked documents and its vectors to the Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52fc805f-ef8f-4bed-a854-9b9cc0e9e6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dictionary to hold and map a book to it's content and page numbers\n",
    "doc_map = {}\n",
    "doc_table_map = {}\n",
    "\n",
    "for doc in container_client.list_blob_names():\n",
    "    print(f\"Extracting content from {doc}...\")\n",
    "\n",
    "    # Capture the start time\n",
    "    start_time = time.time()\n",
    "    url = container_url + \"/\" + doc\n",
    "\n",
    "    # Start extraction\n",
    "    result = extract_pdf(url=url)\n",
    "    page_list = parse_pdf_content(result=result)\n",
    "    doc_name = doc.split(sep=\".\")[0].title()\n",
    "    table_content = [page[\"content\"] for page in page_list if page[\"type\"] == \"table\"][0]\n",
    "    llm_content = get_table_description(table_content=table_content)\n",
    "    page_list[3][\"content\"] = llm_content\n",
    "    doc_map[doc_name] = page_list\n",
    "\n",
    "    # Capture the end time and Calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Parsing took: {elapsed_time:.6f} seconds\")\n",
    "    print(f\"The {doc_name} claim contains {len(page_list)} pages\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c85937-0151-4751-8e09-ab8668b4bc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(search_endpoint, index_name, credential=search_credential)\n",
    "payload_list = []\n",
    "for doc, pagelist in doc_map.items():\n",
    "    for page in pagelist:\n",
    "        try:\n",
    "            id = doc + page[\"content\"][1:10]\n",
    "            title = f\"{doc}\"\n",
    "            upload_payload = {\n",
    "                        \"parent_id\": text_to_base64(text=id),\n",
    "                        \"title\": title,\n",
    "                        \"chunk\": page[\"content\"],\n",
    "                        \"location\": container_url + \"/\" + doc + \".pdf\",\n",
    "                        \"pagenum\": str(page[\"page_number\"]),\n",
    "                        \"vector\": aoai_embeddings.embed_query(page[\"content\"] if page[\"content\"]!=\"\" else \"-------\")\n",
    "            }\n",
    "            payload_list.append(upload_payload)\n",
    "            print(f\"Uploading pages.............for :{doc}\")\n",
    "            result_upload = search_client.upload_documents(documents=[upload_payload])\n",
    "            print(f\"Successfully uploaded pages for :{doc}\")\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a486f2-e423-4a65-b801-ee7c2fb24c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Perform a hybrid search + semantic reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8d3eae-2fac-4c68-bfae-3962f144886d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Semantic Hybrid Search\n",
    "# query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"\n",
    "#query = \"Can you summarize the employee handbook for me in 3 sentences. Use bullet points.\"\n",
    "query = \"How much is the employee's cost per pay check for the north wind standard?\"\n",
    "\n",
    "search_client = SearchClient(search_endpoint, index_name, search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"parent_id\", \"chunk\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=1\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "if semantic_answers:\n",
    "    for answer in semantic_answers:\n",
    "        if answer.highlights:\n",
    "            print(f\"Semantic Answer: {answer.highlights}\")\n",
    "        else:\n",
    "            print(f\"Semantic Answer: {answer.text}\")\n",
    "        print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"parent_id: {result['parent_id']}\")   \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['chunk']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140aeda0-8d67-4e66-b622-4e9fbfcff09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Semantic Hybrid Search\n",
    "# query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"\n",
    "#query = \"Can you summarize the employee handbook for me in 3 sentences. Use bullet points.\"\n",
    "query = \"How much is the employee's cost per pay check for the north wind standard?\"\n",
    "\n",
    "search_client = SearchClient(search_endpoint, index_name, search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"parent_id\", \"content\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=1\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "if semantic_answers:\n",
    "    for answer in semantic_answers:\n",
    "        if answer.highlights:\n",
    "            print(f\"Semantic Answer: {answer.highlights}\")\n",
    "        else:\n",
    "            print(f\"Semantic Answer: {answer.text}\")\n",
    "        print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"parent_id: {result['parent_id']}\")   \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['content']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rag_document_extraction-1.1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
