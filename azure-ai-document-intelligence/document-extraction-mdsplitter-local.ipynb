{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcbfb44-b74a-491d-8718-baf93b6eac05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, AnalyzeDocumentRequest, ContentFormat\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instantiate Keyvault client, then load and set the necessary variables for Azure services.\n",
    "The variables are loaded from Azure Key Vault.\n",
    "\"\"\"\n",
    "\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "\n",
    "keyVaultName = os.environ[\"KEY_VAULT_NAME\"]\n",
    "KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "\n",
    "azure_openai_endpoint=client.get_secret(name=\"aoai-endpoint\").value\n",
    "azure_openai_api_key=client.get_secret(name=\"aoai-api-key\").value\n",
    "azure_openai_api_version = \"2024-02-15-preview\"\n",
    "azure_openai_embedding_deployment = client.get_secret(name=\"aoai-embedding-deployment\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4bd722b-93bb-49a2-95e9-9af8bfa6e2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blobstore00.blob.core.windows.net/document-list\n"
     ]
    }
   ],
   "source": [
    "# Connect to Blob Storage\n",
    "\n",
    "blob_connection_string = client.get_secret(name=\"blobstore-connstr\")\n",
    "#blob_connection_string = dbutils.secrets.get(scope=\"myscope\", key=\"blobstore-connstr\")\n",
    "blob_container_name = \"document-list\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string.value)\n",
    "container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "blobs = container_client.list_blobs()\n",
    "container_url = container_client.url\n",
    "print(container_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec15af13-1f2b-42df-88fd-82d8b78c52b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blobstore00.blob.core.windows.net/document-list/Benefit_Options.pdf\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Length of splits: 6\n",
      "https://blobstore00.blob.core.windows.net/document-list/Northwind_Health_Plus_Benefits_Details.pdf\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Length of splits: 48\n",
      "https://blobstore00.blob.core.windows.net/document-list/Northwind_Standard_Benefits_Details.pdf\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Length of splits: 51\n",
      "https://blobstore00.blob.core.windows.net/document-list/earth_at_night_508.pdf\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Length of splits: 69\n",
      "https://blobstore00.blob.core.windows.net/document-list/sample-layout.pdf\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Length of splits: 2\n"
     ]
    }
   ],
   "source": [
    "# def extract_pdf_document(doc_url: str):\n",
    "book_chunk_map = {}\n",
    "\n",
    "for book in container_client.list_blob_names():\n",
    "    book_url = container_url + \"/\" + book\n",
    "    print(f\"{book_url}\\n\\n\")\n",
    "    print(f\"---------------------------------------------\")\n",
    "    doc_intelligence_endpoint = client.get_secret(name=\"docintelligence-endpoint\")\n",
    "    doc_intelligence_key = client.get_secret(name=\"docintelligence-key\")\n",
    "    # Instantiate the Langchain Azure AI Document Intelligence loader to load the document. You can either specify file_path or url_path to load the document.\n",
    "    # Ensure that the Document Intelligence managed identity is configured with SBDC RBAC on the Blob storage resource.\n",
    "    loader = AzureAIDocumentIntelligenceLoader(url_path=book_url, api_key = doc_intelligence_key.value, api_endpoint = doc_intelligence_endpoint.value, api_model=\"prebuilt-layout\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the document into semantic chunks based on markdown headers, using the MarkdownHeaderTextSplitter class.\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    docs_string = docs[0].page_content\n",
    "    splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "    print(\"Length of splits: \" + str(len(splits)))\n",
    "    #print(f\"{docs_string}\\n\\n\")\n",
    "    #print(f\"{splits}\\n\\n\")\n",
    "    #print(f\"------------------------------------------\")\n",
    "    book_name = book.split(sep=\".\")[0].title()\n",
    "\n",
    "    book_chunk_map[book_name]= splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be6859e-875b-4c18-a063-a9f06210ba90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create and populate an Azure AI Search index with the chunked data from the PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8208fc9-08af-48ee-be7b-5693aee0ecde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the search index fields and vector search configuration\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchField, SearchFieldDataType, VectorSearch, SimpleField, SearchableField, HnswAlgorithmConfiguration, HnswParameters, VectorSearchAlgorithmMetric, ExhaustiveKnnAlgorithmConfiguration, ExhaustiveKnnParameters, VectorSearchProfile, AzureOpenAIVectorizer, AzureOpenAIParameters, SemanticConfiguration, SemanticSearch, SemanticPrioritizedFields, SemanticField, SearchIndex\n",
    "\n",
    "search_credential = AzureKeyCredential(client.get_secret(name=\"aisearch-adminkey\").value)\n",
    "search_endpoint = client.get_secret(name=\"aisearch-endpoint\")\n",
    "# Create a search index client required to create the index\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint.value, credential=search_credential)\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"parent_id\", key=True, type=SearchFieldDataType.String, filterable=True, sortable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True, searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True, sortable=True, facetable=True, retrievable=True),\n",
    "    SearchableField(name=\"location\", type=SearchFieldDataType.String, searchable=True, filterable=True, retrievable=True),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, retrievable=True, hidden=False, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "# Configure the vector search config\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),\n",
    "    ],\n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=azure_openai_endpoint,  \n",
    "                deployment_id=azure_openai_embedding_deployment,  \n",
    "                api_key=azure_openai_api_key,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure semantic search on the index\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        content_fields=[\n",
    "            SemanticField(field_name=\"content\")\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# Create the semantic search config\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dabb6ed-94a6-4306-bd2a-14fa5dab21d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aisearch-index-mdsplitter created\n"
     ]
    }
   ],
   "source": [
    "# Create the search index\n",
    "index_name = \"aisearch-index-mdsplitter\"\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index=index)\n",
    "print(f\"{result.name} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d109d19-39af-4d83-a388-a4182a9e22db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the langchain azure open ai embedding object. This will be used to embed the vector field content\n",
    "# https://python.langchain.com/v0.1/docs/integrations/vectorstores/azuresearch/#create-embeddings-and-vector-store-instances\n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409d37d8-b53c-493a-a084-4ac7694c3591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "def random_text_to_base64(length=16):\n",
    "    # Generate a sequence of random bytes\n",
    "    random_bytes = os.urandom(length)\n",
    "\n",
    "    # Perform URL-safe Base64 encoding\n",
    "    base64_encoded = base64.urlsafe_b64encode(random_bytes)\n",
    "\n",
    "    # Convert the result back to a UTF-8 string representation\n",
    "    base64_text = base64_encoded.decode('utf-8')\n",
    "\n",
    "    return base64_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb388262-2e04-40b1-bc8c-d721268b568d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Upload the semantically chunked documents and its vectors to the Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fd46d3-aae6-4923-9aaa-a8bd7f682976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#book_chunk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "for key, value in book_chunk_map.items():\n",
    "    search_client = SearchClient(search_endpoint, index_name, credential=search_credential)\n",
    "    for doc in value:\n",
    "        try:\n",
    "            content = doc.page_content\n",
    "            book_url = container_url + \"/\" + key + \".pdf\"\n",
    "            parent_id = random_text_to_base64(length=16)\n",
    "            # Ensure parent_id does not start with an underscore\n",
    "            if parent_id.startswith('_'):\n",
    "                parent_id = 'a' + parent_id[1:]  # Replace leading underscore with 'a'\n",
    "            upload_payload = {\n",
    "                        \"parent_id\": parent_id,\n",
    "                        \"title\": key,\n",
    "                        \"content\": content,\n",
    "                        \"location\": book_url,\n",
    "                        \"vector\": aoai_embeddings.embed_query(content if content!=\"\" else \"-------\")\n",
    "            }\n",
    "\n",
    "            result_upload = search_client.upload_documents(documents=[upload_payload])\n",
    "            #print(f\"Successfully uploaded chunk for :{key}\")\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "document-extraction-mdsplitter",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
