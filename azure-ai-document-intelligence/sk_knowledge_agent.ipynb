{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15602fc-75b3-48a2-80c4-592275afee70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## RAG Custom Knowledge AI Agent PoC, with Semantic Kernel.\n",
    "\n",
    "In this notebook, I demonstrate a RAG use case that uses the semantic kernel framework to build an AI knwledge base agent that retrieves relevant content from Azure AI Search and uses the result to augment the query passed to an LLM and generate a response for the company employee user that grounded on the org's knowledge base repository.\n",
    "\n",
    "The agent uses a native function or plugin (in semantic kernel lingo), to make a call to Azure AI Search that ensures the LLM is grounded on relevant and contextual information.\n",
    "\n",
    "Dependencies include: Semantic Kernel python library, Azure AI Search SearchClient package, Azure Open AI python package. Azure blob storage was provisioned as the data store for the PDF documents and Azure AI Document Intelligence is used for document cracking and semantic chunking.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access).\n",
    " \n",
    "+ Azure AI Search, Basic tier, Azure OpenAI.\n",
    "\n",
    "+ A deployment of the `text-embedding-ada-002` and `GPT 4o` models on Azure OpenAI.\n",
    "\n",
    "+ Azure Blob Storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44da0b04-6293-4fc7-85e8-62b2099d478f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Semantic chunking in RAG](https://github.com/jbernec/rag-orchestrations/blob/main/images/semantic-chunking.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc6c320-c964-4da5-b787-5cc7b9d9dffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable\n",
    "from services import Service\n",
    "from samples.service_settings import ServiceSettings\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ac4334-77cc-4b67-8c7b-6166b2896449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def config_kernel() -> Kernel:\n",
    "    pass\n",
    "    # Set the logging level for the kernel to debug\n",
    "    logging.getLogger(name=\"kernel\").setLevel(level=logging.DEBUG)\n",
    "\n",
    "    # Instantiate the kernel object and define the service_id variable\n",
    "    kernel = Kernel()\n",
    "    service_id = dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\")\n",
    "\n",
    "    # Register the AzureChat service with the kernel\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "            endpoint=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-endpoint\"),\n",
    "            api_key=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-api-key\"),\n",
    "            deployment_name=dbutils.secrets.get(\n",
    "                scope=\"myscope\", key=\"aoai-deploymentname\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Register the azure text embedding service\n",
    "    kernel.add_service(\n",
    "        AzureTextEmbedding(\n",
    "            service_id=\"embedding\",\n",
    "            endpoint=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-endpoint\"),\n",
    "            deployment_name=\"embedding\",\n",
    "            api_key=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-api-key\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f322d7-f8d6-44bb-83e2-288c3aa80984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUn the kernel instantiation function\n",
    "kernel = await config_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00313305-d3ac-4b3d-b138-0cae586dec82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import memory store packages\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a104f03a-af23-4dbd-a49c-2afef2923c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def config_skmemory():\n",
    "    # instantiate the azure ai search store\n",
    "    ai_search_store = AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=1536,\n",
    "        search_endpoint=dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-endpoint\"),\n",
    "        admin_key=dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-adminkey\")\n",
    "    )\n",
    "\n",
    "    # instantiate the semantic memory abstraction with the ai search store\n",
    "    embedding_gen = AzureTextEmbedding(\n",
    "        service_id=\"embedding\",\n",
    "        endpoint=\"https://aoai-srv.openai.azure.com/\",\n",
    "        deployment_name=\"embedding\",\n",
    "        api_key=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-api-key\"),\n",
    "    )\n",
    "\n",
    "    memory = SemanticTextMemory(\n",
    "        storage=ai_search_store,\n",
    "        embeddings_generator=embedding_gen\n",
    "    )\n",
    "\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35777f4a-ce75-49bf-97ee-2183e310d7bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute the memory function\n",
    "memory = await config_skmemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1c5fa3-1659-4283-b518-78c01bddf2b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Add Semantic search to the Chatbot to enable a RAG workflow based on internal company documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999dbf58-6671-4163-b0f4-4fc2c737a388",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant powered by the ChatGPT-4 model. Your task is to respond to user queries based on the provided input text, the history of the conversation, and the context derived from a retrieval search system. While your responses should be primarily grounded in the context, you can offer limited suggestions outside of the context if they are relevant and beneficial to the user.\n",
    "\n",
    "**Input Variables:**\n",
    "1. **User Input Text:** {{$user_input}}\n",
    "2. **Chat History:** {{$chat_history}}\n",
    "3. **Context from Retrieval System:** {{$retrieval_context}}\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the **User Input Text** to understand the current query or request.\n",
    "2. Refer to the **Chat History** to maintain coherence and continuity in the conversation.\n",
    "3. Utilize the **Context from Retrieval System** to provide accurate and contextually relevant responses.\n",
    "4. If the context does not fully address the user's needs, provide limited and relevant suggestions based on general knowledge or logical inference.\n",
    "5. Ensure responses are clear, concise, and helpful.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**User Input Text:** \"What is the capital of France?\"\n",
    "**Chat History:** \"User previously asked about European countries and their capitals.\"\n",
    "**Context from Retrieval System:** \"France is a country in Europe with Paris as its capital.\"\n",
    "\n",
    "**Response:** \"The capital of France is Paris. If you have any other questions about European countries or need more information, feel free to ask!\"\n",
    "\n",
    "**Template:**\n",
    "\n",
    "**User Input Text:** \"{{$user_input}}\"\n",
    "**Chat History:** \"{{$chat_history}}\"\n",
    "**Context from Retrieval System:** \"{{$retrieval_context}}\"\n",
    "\n",
    "**Response:** [Your response here]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc00f706-d7e2-443d-a5ad-799685007601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import search related packages\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.search.documents.models import (\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Assign values to these variables using the corresponding azure key vault secrets\n",
    "endpoint = dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-endpoint\")\n",
    "credential = AzureKeyCredential(dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-adminkey\")) if len(dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-adminkey\")) > 0 else DefaultAzureCredential()\n",
    "index_name = \"manual-aisearch-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc65b34-b285-4e26-b81a-cd8c401a7ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Develop a class plugin. In this class, define functions that will generate a random number between a min and max int that will be used as the number of paragraphs n for a semantic function that generates a short story.\n",
    "# Also, define a function that will search the azure ai search vector db for relevant answers to user prompts\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "from semantic_kernel.functions import kernel_function\n",
    "import random\n",
    "\n",
    "class SearchRetrievalPlugin:\n",
    "    \"\"\"\n",
    "    Description: Query the Azure AI Search Vector DB for a context specific answer.\n",
    "    \"\"\"\n",
    "    @kernel_function(\n",
    "        description=\"Search retrieval function\", name=\"search_retrieval\"\n",
    "    )\n",
    "    def search_retrieval(self, user_input:str) -> str:\n",
    "        \"\"\"\n",
    "        Search and retrieve answers from Azure AI Search.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        query = user_input\n",
    "        search_client = SearchClient(endpoint, index_name, credential)\n",
    "        vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "        r = search_client.search(  \n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"parent_id\", \"content\"],\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name='my-semantic-config',\n",
    "        query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "        query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "        top=1\n",
    "    )\n",
    "        #query_result = results.get_answers()[0].text\n",
    "        results = [doc[\"content\"].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "        content = \"\\n\".join(results)\n",
    "        return content\n",
    "    \n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Generate a random number between min and max\", name=\"generate_number\"\n",
    "    )\n",
    "\n",
    "    def generate_number(self, min: Annotated[int, \"minimum number of paragraphs\"], max: Annotated[int, \"maximum number of paragraphs\"] = 10) -> Annotated[int, \"output is a number\"]:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min='4' max='10' => randint(4,19)\n",
    "        Args:\n",
    "            min - The lower limit for the random number generation\n",
    "            max - The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int - value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(min, max))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {min} and {max}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27c0d08-6aca-44fb-8631-b914f829bd76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def chat_func_acs_memory(kernel: Kernel, memory: SemanticTextMemory):\n",
    "\n",
    "    kernel.add_plugin(plugin=TextMemoryPlugin(memory=memory), plugin_name=\"TextMemoryAISearchPlugin\")\n",
    "\n",
    "    # define execution execution settings\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "        ai_model_id=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    chat_template_config = PromptTemplateConfig(\n",
    "        template=prompt,\n",
    "        description=\"Chat with the assistant\",\n",
    "        input_variables=[\n",
    "            InputVariable(\n",
    "                name=\"user_input\", description=\"The user input\", is_required=True\n",
    "            ),\n",
    "            InputVariable(\n",
    "                name=\"chat_history\",\n",
    "                description=\"The history of the conversation\",\n",
    "                is_required=True,\n",
    "            ),\n",
    "            InputVariable(\n",
    "                name=\"retrieval_context\",\n",
    "                description=\"The memory search query\",\n",
    "                is_required=True,\n",
    "            ),\n",
    "        ],\n",
    "        execution_settings=execution_settings,\n",
    "    )\n",
    "\n",
    "    chat_func = kernel.add_function(\n",
    "        prompt=prompt,\n",
    "        function_name=\"chatFunction\",\n",
    "        plugin_name=\"chatPlugin\",\n",
    "        description=\"chat with assistant\",\n",
    "        prompt_template_config=chat_template_config,\n",
    "    )\n",
    "\n",
    "    # Register the plugin class with the kernel\n",
    "    search_retrieval_plugin = kernel.add_plugin(SearchRetrievalPlugin(), plugin_name=\"SearchRetrievalPlugin\")\n",
    "\n",
    "    # extract and hold each native function in it's own variable object for further use\n",
    "    search_retrieval = search_retrieval_plugin.get(\"search_retrieval\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"User:>\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nExiting chat..\")\n",
    "            return False\n",
    "        except EOFError:\n",
    "            print(\"\\n\\nExiting chat..\")\n",
    "            return False\n",
    "        if user_input == \"exit\":\n",
    "            print(\"\\n\\nExiting chat..\")\n",
    "            return \"Good bye, please let me know if you need further help.\"\n",
    "        if user_input == \"quit\":\n",
    "            print(\"\\n\\nExiting chat..\")\n",
    "            return \"Good bye, please let me know if you need further help.\"\n",
    "\n",
    "        # read the user's chat message, add it to the chat history, add the AI's reply to our chat history\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(user_input)\n",
    "        # invoke the plugin function\n",
    "        content = await search_retrieval.invoke(kernel=kernel, user_input=user_input)\n",
    "        arguments = KernelArguments(chat_history=chat_history, user_input=user_input, retrieval_context=content)\n",
    "        response = await kernel.invoke(function=chat_func, arguments=arguments)\n",
    "        chat_history.add_assistant_message(str(response))\n",
    "        print(f\"Assistant:> {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35f09bf-9020-4b5e-b0e5-5acd34c213a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run the chat function\n",
    "await chat_func_acs_memory(kernel=kernel, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4174058a-0eca-4ce2-b064-867d4048ba5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### The combination of semantic chunking and the prebuilt layout model of Azure AI Document Intelligence provided more relevant and accurate response to all 5 questions asked. Especially the question relating to the cost of heathcare. \n",
    "\n",
    "##### The previous solution developed with the Azure AI Search integrated vectorization failed to produce accurate and relevant answers relating to cost. This is because the cost information is contained in an embedded table in the PDF document. The native document extraction model defined as part of the integrated vectorization was unable to successfuly crack and extract all relevant details from the document.\n",
    "\n",
    "##### In addition, semantic chunking ensured that all answers remained relevant to the user prompt."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sk_knowledge_agent",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
