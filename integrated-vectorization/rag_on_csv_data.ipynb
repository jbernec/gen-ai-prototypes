{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36bb9bfe-65b0-49ae-bff9-f8f2a45ec9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### In this notebook I demonstrate the use of the Azure AI Search Integrated Vectorization feature. and the Split Skill and Azure Open AI Embedding skill to index and build an agentic RAG solution on a glossary dataset in the CSV file format.\n",
    "* Key vault retrievals are implemented using the azure key vault sdk\n",
    "* The data source object connection string parameter was updated to use the storage account resource id: ResourceId=/subscriptions/00000000-0000-8888-7777-555555555555/resourceGroups/rgxxx/providers/Microsoft.Storage/storageAccounts/blob00000store\n",
    "* The NativeBlobSoftDeleteDeletionDetectionPolicy is not supported for parsingMode indexer config set to delimitedText. Further research required\n",
    "* It's important that the requirements.txt file pinned packages in this directory are used, to avoid breaking changes in newer versions for now\n",
    "* https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-document-intelligence-layout\n",
    "* https://learn.microsoft.com/en-us/azure/search/search-how-to-semantic-chunking\n",
    "* SplitSkill to chunk the data\n",
    "* AzureOpenAIEmbedding skill to embed the dataset \"Definition\" field \n",
    "* Deploy the following services in the same region; Azure AI Document Intelligence, Azure AI Search, Azure Open AI, AI Foundry, Azure Blob Storage\n",
    "* Enable system assigned managed identity\n",
    "* Deploy text-embedding-3-small on Azure OpenAI (in Azure AI Foundry) for embeddings\n",
    "* Deploy gpt-4o on Azure OpenAI for chat completion\n",
    "* Configure search engine RBAC to Azure Blob Storage by adding a role for Storage Blob Data Reader, assigned to the search service system-managed identity\n",
    "* Configure search engine RBAC to Azure Open AI by adding a role for Cognitive Services OpenAI User, assigned to the search service system-managed identity\n",
    "* The model names and endpoint should be saved in AKV. Embedding skills and vectorizers assemble the full endpoint internally, so only the resource URI is needed. For example, given https://MY-FAKE-ACCOUNT.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-06-01, the endpoint should to be provided in skill and vectorizer definitions is https://MY-FAKE-ACCOUNT.openai.azure.com.\n",
    "* The Azure AI multiservice account is used for skills processing. The multiservice account key must be provided, even if RBAC is in use. The key isn't used on the connection, but it's currently used for billing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b49d8507-cef3-4dd7-a839-90785e45efef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7131c8-493d-4ec8-aa00-92245f8f6035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import base64\n",
    "from openai import AzureOpenAI\n",
    "import azure.identity\n",
    "from azure.identity import DefaultAzureCredential, EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, AnalyzeDocumentRequest, ContentFormat\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex,\n",
    "    BlobIndexerParsingMode,\n",
    "    SemanticConfiguration, SemanticSearch, SemanticPrioritizedFields, SemanticField\n",
    ")\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "import os\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.identity import DefaultAzureCredential, AzureAuthorityHosts\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ed4eaa-5b15-4477-bb61-d0ef8a804d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define Required Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    keyVaultName = os.environ[\"KEY_VAULT_NAME\"]\n",
    "except KeyError:\n",
    "    # Get input from user if not set\n",
    "    keyVaultName = input(\"Please enter your Key Vault name: \")\n",
    "    # Save for future cells in this session\n",
    "    os.environ[\"KEY_VAULT_NAME\"] = keyVaultName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyVaultName = os.environ[\"KEY_VAULT_NAME\"]\n",
    "KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = SecretClient(vault_url=KVUri, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a658011-70c3-4a56-a0cf-6aed3b154194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads and sets the necessary variables for Azure services.\n",
    "The variables are loaded from Azure Key Vault.\n",
    "\"\"\"\n",
    "azure_openai_endpoint=client.get_secret(name=\"aoai-endpoint\").value\n",
    "\n",
    "azure_openai_api_key=client.get_secret(name=\"aoai-api-key\").value\n",
    "azure_openai_api_version = \"2024-02-15-preview\"\n",
    "azure_openai_embedding_deployment = client.get_secret(name=\"aoai-embedding-deployment\").value\n",
    "azure_openai_embedding_model =client.get_secret(name=\"aoai-embedding-model\").value\n",
    "azure_openai_vector_dimension = 1536\n",
    "search_credential =AzureKeyCredential(client.get_secret(name=\"aisearch-key\").value)\n",
    "search_endpoint =client.get_secret(name=\"aisearch-endpoint\").value\n",
    "index_name = \"csv-glossary-index\"\n",
    "data_source_connection_name = \"csv-glossary-ds\"\n",
    "azure_ai_services_key =client.get_secret(name=\"azure-ai-services-key\").value\n",
    "azure_ai_services_endpoint =client.get_secret(name=\"azure-ai-services-endpoint\").value\n",
    "blob_container_name = \"excel-data\"\n",
    "blob_storage_name =client.get_secret(name=\"blobstore-account-name\").value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865392b6-e124-4ac8-85d6-320c8489743a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Azure AI Search Datasource Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc31b92d-7ca5-475c-8544-5c8ceaaa224f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataIdentity,\n",
    "    SearchIndexerDataUserAssignedIdentity,\n",
    "    SearchIndexerDataNoneIdentity\n",
    ")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    NativeBlobSoftDeleteDeletionDetectionPolicy,\n",
    "    HighWaterMarkChangeDetectionPolicy,\n",
    "    DataChangeDetectionPolicy\n",
    ")\n",
    "\n",
    "indexer_client = SearchIndexerClient(\n",
    "    endpoint=search_endpoint, credential=search_credential\n",
    ")\n",
    "indexer_container = SearchIndexerDataContainer(name=blob_container_name)\n",
    "resource_id = client.get_secret(name=\"ds-resource-id\").value\n",
    "data_source_connection = SearchIndexerDataSourceConnection(name=data_source_connection_name, type=\"azureblob\", connection_string=resource_id, container=indexer_container)\n",
    "\n",
    "# Create the data source object\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection=data_source_connection)\n",
    "\n",
    "print(f\"Data source {data_source.name} created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'csv-glossary-ds'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azure.search.documents.indexes.models._models.SearchIndexerDataSourceConnection"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd4c984-9d0e-4c65-90b8-b830be4727e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3bc09c0-3339-4c83-a38b-5719184a7093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SearchField(name=\"id\",type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"parent_id\",type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"title\",type=SearchFieldDataType.String, searchable=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"category\",type=SearchFieldDataType.String, searchable=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"notes\",type=SearchFieldDataType.String, searchable=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"chunk\",type=SearchFieldDataType.String, searchable=True, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"vector\",type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=azure_openai_vector_dimension, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "\n",
    "# Define the vector search configuration and parameters\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(name=\"myHsnw\")\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHsnw\",\n",
    "            vectorizer_name=\"myOpenAI\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myOpenAI\",\n",
    "            kind=\"azureOpenAI\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_deployment,\n",
    "                model_name=azure_openai_embedding_model,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure semantic search on the index\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic search config\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "scoring_profiles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c35313-e690-40ca-ba9f-d25a4339737e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv-glossary-index created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index client required to create the index\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, scoring_profiles=scoring_profiles, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index=index)\n",
    "print(f\"{result.name} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e26ae82-b593-43c7-ab2d-2931bd7cd642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> #### Create Required Skillsets for the document extraction processes and operations.\n",
    "Skills drive integrated vectorization. Text Split provides data chunking. AzureOpenAIEmbedding handles calls to Azure OpenAI, using the connection information you provide in the environment variables. An indexer projection specifies secondary indexes used for chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd41d6f0-bc50-4ab6-adee-1037af8e6c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created skillset csv-glossary-index-skillset\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    OcrSkill,\n",
    "    SearchIndexerSkillset,\n",
    "    DocumentIntelligenceLayoutSkill,\n",
    "    DocumentIntelligenceLayoutSkillMarkdownHeaderDepth,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    AIServicesAccountKey,\n",
    "    AIServicesAccountIdentity\n",
    ")\n",
    "\n",
    "skillset_name = f\"{index_name}-skillset\"\n",
    "\n",
    "def create_skillset():\n",
    "\n",
    "    split_skill = SplitSkill(\n",
    "        description=\"Split skill to chunk documents\",\n",
    "        text_split_mode=\"pages\",\n",
    "        default_language_code=\"en\",\n",
    "        context=\"/document\",\n",
    "        maximum_page_length=2000,\n",
    "        page_overlap_length=500,\n",
    "        inputs=[\n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/Definition\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            OutputFieldMappingEntry(name=\"textItems\", target_name=\"chunks\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    embedding_skill = AzureOpenAIEmbeddingSkill(\n",
    "        description=\"Skill to generate embeddings via Azure OpenAI\",\n",
    "        context=\"/document/chunks/*\",\n",
    "        resource_url=azure_openai_endpoint,\n",
    "        deployment_name=azure_openai_embedding_deployment,\n",
    "        model_name=azure_openai_embedding_model,\n",
    "        dimensions=azure_openai_vector_dimension,\n",
    "        api_key=azure_openai_api_key,\n",
    "        inputs=[\n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/chunks/*\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    index_projections = SearchIndexerIndexProjection(\n",
    "        selectors=[\n",
    "            SearchIndexerIndexProjectionSelector(\n",
    "                target_index_name=index_name,\n",
    "                parent_key_field_name=\"parent_id\",\n",
    "                source_context=\"/document/chunks/*\",\n",
    "                mappings=[\n",
    "                    InputFieldMappingEntry(name=\"chunk\", source=\"/document/chunks/*\"),\n",
    "                    InputFieldMappingEntry(name=\"vector\", source=\"/document/chunks/*/vector\"),\n",
    "                    InputFieldMappingEntry(name=\"title\", source=\"/document/Term\"),\n",
    "                    InputFieldMappingEntry(name=\"category\", source=\"/document/Category\"),\n",
    "                    InputFieldMappingEntry(name=\"notes\", source=\"/document/Notes\"),\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        parameters=SearchIndexerIndexProjectionsParameters(\n",
    "            projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    skills = [split_skill, embedding_skill]\n",
    "\n",
    "    return SearchIndexerSkillset(\n",
    "        name=skillset_name,\n",
    "        description=\"Skillset to chunk documents and generating embeddings\",\n",
    "        skills=skills,\n",
    "        index_projection=index_projections,\n",
    "        cognitive_services_account=AIServicesAccountKey(key=azure_ai_services_key, subdomain_url=azure_ai_services_endpoint)\n",
    "    )\n",
    "\n",
    "skillset = create_skillset()\n",
    "\n",
    "\n",
    "indexer_client.create_or_update_skillset(skillset)\n",
    "print(f\"Created skillset {skillset.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4c4574-5ae6-4ad1-ad5f-d2a37a2cd69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7008d8d8-dc12-4b0d-9a18-73f2020d38e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " csv-glossary-index-indexer is created and running. If queries return no results, please wait a bit and try again.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    BlobIndexerImageAction\n",
    ")\n",
    "\n",
    "# Define indexer name  \n",
    "indexer_name = f\"{index_name}-indexer\"\n",
    "\n",
    "index_parameters = IndexingParameters(\n",
    "    configuration=IndexingParametersConfiguration(\n",
    "      data_to_extract=\"contentAndMetadata\",\n",
    "      parsing_mode=\"delimitedText\",\n",
    "      first_line_contains_headers=True,\n",
    "      query_timeout=None,\n",
    "    )\n",
    "  )\n",
    "\n",
    "indexer = SearchIndexer(\n",
    "  name=indexer_name,\n",
    "  description=\"Indexer to orchestrate the document indexing and embedding generation\",\n",
    "  skillset_name=skillset_name,\n",
    "  target_index_name=index_name,\n",
    "  data_source_name=data_source.name,\n",
    "  parameters=index_parameters\n",
    ")\n",
    "\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)\n",
    "\n",
    "# Run the indexer to kick off the indexing process\n",
    "indexer_client.run_indexer(indexer_name)\n",
    "print(f' {indexer_name} is created and running. If queries return no results, please wait a bit and try again.')\n",
    "\n",
    "# Schedule an indexer to run every 24 hours\n",
    "#https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_indexer_datasource_skillset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c659d974-38e9-490e-b5cb-051039d2282f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Perform a vector similarity search\n",
    "\n",
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization.\n",
    "\n",
    "If you indexed the health plan PDF file, send queries that ask plan-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b9c6be-02d7-479c-ba38-526c1be978e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 99d593c454b5_aHR0cHM6Ly9ibG9ic3RvcmUwNS5ibG9iLmNvcmUud2luZG93cy5uZXQvZXhjZWwtZGF0YS9nbG9zc2FyeV9kYXRhc2V0LmNzdjsx0_chunks_0\n",
      "notes: Used in web development to connect services.\n",
      "Score: 0.68176585\n",
      "Content: Application Programming Interface: a set of rules that allows different software entities to interact.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "# query = \"What can you tell me about application programming interface ?\"\n",
    "query = \"what is an API ?\"\n",
    "\n",
    "search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"id: {result['id']}\")  \n",
    "    print(f\"notes: {result['notes']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5bba0c-aee8-46a3-b11a-1bdc9053d8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 99d593c454b5_aHR0cHM6Ly9ibG9ic3RvcmUwNS5ibG9iLmNvcmUud2luZG93cy5uZXQvZXhjZWwtZGF0YS9nbG9zc2FyeV9kYXRhc2V0LmNzdjs50_chunks_0\n",
      "notes: Common in Scrum methodology.\n",
      "Score: 0.6210777\n",
      "Content: A short, time-boxed period in which a scrum team works to complete a set amount of work.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"What's the difference between a sprint and agile?\"\n",
    "\n",
    "search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"id: {result['id']}\")  \n",
    "    print(f\"notes: {result['notes']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063ffd90-bfc0-411f-8c91-e85234c8175f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Perform a hybrid search + semantic reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c62369-e119-429a-a54c-2f3dd84592da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: <em>A short, time-boxed period in which a scrum team works to complete a set amount of work.</em>\n",
      "Semantic Answer Score: 0.9850000143051147\n",
      "\n",
      "id: 99d593c454b5_aHR0cHM6Ly9ibG9ic3RvcmUwNS5ibG9iLmNvcmUud2luZG93cy5uZXQvZXhjZWwtZGF0YS9nbG9zc2FyeV9kYXRhc2V0LmNzdjs50_chunks_0\n",
      "notes: Common in Scrum methodology.\n",
      "Reranker Score: 2.586292028427124\n",
      "Content: A short, time-boxed period in which a scrum team works to complete a set amount of work.\n",
      "Caption: <em>A short, time-boxed period in which a scrum team works to complete a set amount of work.</em>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import (\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "# Semantic Hybrid Search\n",
    "query = \"What's the difference between a sprint and agile?\"\n",
    "\n",
    "search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=2, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"id\", \"notes\", \"chunk\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=1\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "if semantic_answers:\n",
    "    for answer in semantic_answers:\n",
    "        if answer.highlights:\n",
    "            print(f\"Semantic Answer: {answer.highlights}\")\n",
    "        else:\n",
    "            print(f\"Semantic Answer: {answer.text}\")\n",
    "        print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"id: {result['id']}\")  \n",
    "    print(f\"notes: {result['notes']}\")  \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['chunk']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rag_on_csv_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
