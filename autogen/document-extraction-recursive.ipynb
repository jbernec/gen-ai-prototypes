{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b676ea-24ba-4956-b322-0e3b50122965",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Key Points:\n",
    "#####This implementation is part of a PoC to evaluate the azure ai document itelligence sdk for complex document extraction. This notebook tests a single page table extraction and associated text and uses the recursive character text splitter class for chunking. The output is uploaded to azure ai search and used to power a q & a knowledgebase ai chat application and enhance response accuracy and relevance.\n",
    "#####Extract Table Function: The extract_tables function is in in development and will be designed to extract a multi-page table data. It identifies the column headers and row indexes, then extracts the cell values and adds them to a string.\n",
    "#####Extract PDF: The extract_pdf_content function loops through PDF books in a storage account container, then extracts text and single-page table content into a markdown format by min span offset and maxt span length. The extracted content is chunked using the recursive character text splitter class via a specified number of characters or separators.\n",
    "https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/data-chunking/langchain-data-chunking-example.ipynb\n",
    "Chunking Strategies: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-chunking-phase\n",
    "\n",
    "Example: https://learn.microsoft.com/en-us/answers/questions/1608976/using-document-intelligence-to-create-chunks-for-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcbfb44-b74a-491d-8718-baf93b6eac05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, AnalyzeDocumentRequest, ContentFormat\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import get_bearer_token_provider\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from time import monotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67a5a2e-8cb6-4844-97dd-a0b58dc2c122",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads and sets the necessary variables for Azure services.\n",
    "The variables are loaded from Azure Key Vault.\n",
    "\"\"\"\n",
    "\n",
    "azure_openai_endpoint=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-endpoint\")\n",
    "azure_openai_api_key=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-api-key\")\n",
    "azure_openai_api_version = \"2024-02-15-preview\"\n",
    "azure_openai_embedding_deployment = dbutils.secrets.get(scope=\"myscope\", key=\"aoai-embedding-deployment\")\n",
    "doc_intelligence_endpoint = dbutils.secrets.get(scope=\"myscope\", key=\"docintelligence-endpoint\")\n",
    "doc_intelligence_key = dbutils.secrets.get(scope=\"myscope\", key=\"docintelligence-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4bd722b-93bb-49a2-95e9-9af8bfa6e2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Blob Storage\n",
    "blob_connection_string = dbutils.secrets.get(scope=\"myscope\", key=\"blobstore-connstr\")\n",
    "blob_container_name = \"document-list\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "blobs = container_client.list_blobs()\n",
    "container_url = container_client.url\n",
    "#print(container_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71310bbe-facb-4164-951c-e22cd5b2bc34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract tables from the page\n",
    "def extract_tables(result: AnalyzeResult):\n",
    "    tables = []\n",
    "    for page in result.pages:\n",
    "        for table in result.tables:\n",
    "            if page.page_number == table.bounding_regions[0].page_number:\n",
    "                table_data = []\n",
    "                for cell in table.cells:\n",
    "                    table_data.append({\n",
    "                        \"row_index\": cell.row_index,\n",
    "                        \"column_index\": cell.column_index,\n",
    "                        \"content\": cell.content\n",
    "                    })\n",
    "                tables.append(table_data)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a958e0d-520d-4e1a-864e-32109d7afca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Function to convert text to unique random id for search index field\n",
    "def text_to_base64(text):\n",
    "    # Convert text to bytes using UTF-8 encoding\n",
    "    bytes_data = text.encode('utf-8')\n",
    "\n",
    "    # Perform Base64 encoding\n",
    "    base64_encoded = base64.b64encode(bytes_data)\n",
    "\n",
    "    # Convert the result back to a UTF-8 string representation\n",
    "    base64_text = base64_encoded.decode('utf-8')\n",
    "\n",
    "    return base64_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfdf5d2a-a395-4512-bd37-f1b348a673ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "page_documents = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090e0432-c285-4c19-a0dc-bf3420fab375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(page_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474ad706-1eff-4786-ae43-234e07feb670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to crack and extract PDF documents using Azure AI Document Intelligence\n",
    "def extract_pdf_content(book_url: str):\n",
    "    page_documents = \"\"\n",
    "    print(f\"{book_url}\\n\\n\")\n",
    "    print(f\"---------------------------------------------\")\n",
    "    \n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=doc_intelligence_endpoint, credential=AzureKeyCredential(key=doc_intelligence_key))\n",
    "\n",
    "    poller= document_intelligence_client.begin_analyze_document(model_id=\"prebuilt-layout\", analyze_request=AnalyzeDocumentRequest(url_source=book_url), output_content_format=\"markdown\")\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    \n",
    "    for page in result.pages:\n",
    "        page_num = page.page_number\n",
    "        # Calculate the start position as the offset of the first span\n",
    "        start_pos = page.spans[0].offset\n",
    "\n",
    "        # Calculate the end position by adding the length of the first span to its offset\n",
    "        end_pos = start_pos + page.spans[0].length\n",
    "\n",
    "        # Slice the result.content string from start_pos to end_pos to get the desired content\n",
    "        page_content = result.content[start_pos:end_pos]\n",
    "        #print(f\"{page_content}\\n\\n\")\n",
    "\n",
    "        #print(f\"------------------------------------------\")\n",
    "        page_documents+=page_content\n",
    "\n",
    "    \n",
    "    return page_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c97bd69-8f2f-40d6-810c-c1ee279ceb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the search index fields and vector search configuration\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchField, SearchFieldDataType, VectorSearch, SimpleField, SearchableField, HnswAlgorithmConfiguration, HnswParameters, VectorSearchAlgorithmMetric, ExhaustiveKnnAlgorithmConfiguration, ExhaustiveKnnParameters, VectorSearchProfile, AzureOpenAIVectorizer, AzureOpenAIParameters, SemanticConfiguration, SemanticSearch, SemanticPrioritizedFields, SemanticField, SearchIndex\n",
    "\n",
    "search_credential = AzureKeyCredential(dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-adminkey\"))\n",
    "search_endpoint = dbutils.secrets.get(scope=\"myscope\", key=\"aisearch-endpoint\")\n",
    "# Create a search index client required to create the index\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", key=True, type=SearchFieldDataType.String, filterable=True, sortable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True, searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True, sortable=True, facetable=True, retrievable=True),\n",
    "    SearchableField(name=\"location\", type=SearchFieldDataType.String, searchable=True, filterable=True, retrievable=True),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, retrievable=True, hidden=False, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "# Configure the vector search config\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),\n",
    "    ],\n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=azure_openai_endpoint,  \n",
    "                deployment_id=azure_openai_embedding_deployment,  \n",
    "                api_key=azure_openai_api_key,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure semantic search on the index\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        content_fields=[\n",
    "            SemanticField(field_name=\"content\")\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# Create the semantic search config\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01b0772-c435-4437-88d0-b205e9459667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the search index\n",
    "index_name = \"aisearch-index-recursive\"\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index=index)\n",
    "print(f\"{result.name} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1a70a7-b8c2-4f6f-90c9-65654c45899e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the langchain azure open ai embedding object. This will be used to embed the vector field content\n",
    "# https://python.langchain.com/v0.1/docs/integrations/vectorstores/azuresearch/#create-embeddings-and-vector-store-instances\n",
    "\n",
    "# Create azure open ai embedding\n",
    "azure_openai_client = None\n",
    "if azure_openai_api_key:\n",
    "    azure_openai_client = AzureOpenAI(\n",
    "        api_key=azure_openai_api_key, \n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_deployment=azure_openai_embedding_deployment,\n",
    "        azure_endpoint=azure_openai_endpoint)\n",
    "else:\n",
    "    azure_openai_client = AzureOpenAI(\n",
    "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), scope=\"https://cognitiveservices.azure.com/.default\"),\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_deployment=azure_openai_embedding_deployment,\n",
    "        azure_endpoint=azure_openai_endpoint)\n",
    "    \n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bf942c-4fbf-42ef-95ce-ab77454a9793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text(text: str):\n",
    "    pass\n",
    "    recursive_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=125,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    recursive_text_splitter_chunks = recursive_text_splitter.split_text(text=text)\n",
    "    return recursive_text_splitter_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706022a9-95b5-4447-928f-5cbb23ecf039",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dictionary to hold and map a book to it's content and page numbers\n",
    "book_pages_map = {}\n",
    "\n",
    "for book in container_client.list_blob_names():\n",
    "    print(f\"Extracting content from {book}...\")\n",
    "\n",
    "    # Capture the start time\n",
    "    start_time = time.time()\n",
    "    book_url = container_url + \"/\" + book\n",
    "\n",
    "    # Start extraction\n",
    "    page_documents = extract_pdf_content(book_url=book_url)\n",
    "    book_name = book.split(sep=\".\")[0].title()\n",
    "    chunks = chunk_text(page_documents)\n",
    "    #chunked_docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "    book_pages_map[book_name]= chunks\n",
    "\n",
    "    # Capture the end time and Calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Parsing took: {elapsed_time:.6f} seconds\")\n",
    "    print(f\"The {book_name} book contains {len(chunks)} chunks\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109af4c7-296f-40fd-97a5-0fa9c222aec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa9e0f0-6059-4215-a207-9c65198c10a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(search_endpoint, index_name, credential=search_credential)\n",
    "\n",
    "for bookname, chunks in book_pages_map.items():\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            id = bookname + chunk[1:10]\n",
    "            title = f\"{bookname}\"\n",
    "            upload_payload = {\n",
    "                        \"id\": text_to_base64(text=id),\n",
    "                        \"title\": title,\n",
    "                        \"content\": chunk,\n",
    "                        \"location\": container_url + \"/\" + bookname + \".pdf\",\n",
    "                        \"vector\": aoai_embeddings.embed_query(chunk if chunk!=\"\" else \"-------\")\n",
    "            }\n",
    "\n",
    "            result_upload = search_client.upload_documents(documents=[upload_payload])\n",
    "            #print(f\"Successfully uploaded chunk for :{bookname}\")\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cac1e7-b850-44a1-ba6b-f93b3b2d3b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_pages_map.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bba2846-a6e0-4f1c-9efe-21aa0f1e1902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test Specific Search Types and Queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fec17b7-eab5-48d0-b8df-6a77170d0e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8688fe92-7e79-419d-bacd-43f07103963b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization.\n",
    "\n",
    "If you indexed the health plan PDF file, send queries that ask plan-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d827688-d5fb-4d95-a485-4448fab2e0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "# query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"\n",
    "query = \"What determines the venue of a legal action brought against Northwind Health?\"  \n",
    "  \n",
    "search_client = SearchClient(search_endpoint, index_name, credential=search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"id\", \"title\", \"content\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ae5926-f836-42dd-867b-015e97313daa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Perform a hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f80958-9970-401d-89de-a69055de5862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Hybrid Search query\n",
    "#query = \"How much is the employee's cost per pay check for the north wind standard?\"  \n",
    "# query = \"Can you summarize the employee handbook for me?\"\n",
    "query = \"What determines the venue of a legal action brought against Northwind Health?\"\n",
    "  \n",
    "search_client = SearchClient(search_endpoint, index_name, credential=search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  # use both the text query\n",
    "    vector_queries= [vector_query], # use both the text query in the previous parameter and vector query\n",
    "    select=[\"id\", \"title\", \"content\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb1a93c-04fa-4e1e-afe0-ce3da2041a11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Perform a hybrid search + semantic reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afad53a0-fd7e-41e7-878e-2626314d660e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType\n",
    ")\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Semantic Hybrid Search\n",
    "# query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"\n",
    "query = \"What determines the venue of a legal action brought against Northwind Health?\"\n",
    "\n",
    "search_client = SearchClient(search_endpoint, index_name, search_credential)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"id\", \"title\", \"content\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=1\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "if semantic_answers:\n",
    "    for answer in semantic_answers:\n",
    "        if answer.highlights:\n",
    "            print(f\"Semantic Answer: {answer.highlights}\")\n",
    "        else:\n",
    "            print(f\"Semantic Answer: {answer.text}\")\n",
    "        print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Id: {result['id']}\")  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['content']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "document-extraction-recursive",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
