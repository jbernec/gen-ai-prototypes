{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0ce00a-ed5a-4d02-a32d-9977387da2ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build an Autogen agent that can scrape a website for data and format the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef09bd7-b065-4384-a2b0-105c5e3a3a4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1: Import required packages and set required variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c525c5-cf08-4756-a5de-ca8712aab1f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import autogen\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import Annotated\n",
    "from autogen import ConversableAgent, register_function\n",
    "\n",
    "# Instead of using the autogen.config_list_from_json function to load a list of llms and their ppties from a json/text file, I'll just define an llm_config dictionary variable in the notebook.\n",
    "\n",
    "llm_config = {\n",
    "  \"config_list\": [\n",
    "    {\n",
    "      \"model\": dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "      \"api_key\": dbutils.secrets.get(scope=\"myscope\", key=\"aoai-api-key\"),\n",
    "      \"base_url\": dbutils.secrets.get(scope=\"myscope\", key=\"aoai-endpoint\"),\n",
    "      \"api_type\": \"azure\",\n",
    "      \"api_version\": \"2024-02-15-preview\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c563eb7c-416d-449c-8cd7-04074d1fc2a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 2: Define a function that'll scrape content from a blog post using the apify client actor method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823da5e1-f990-47cf-a411-ffda88bc0644",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scrape_blog(url: Annotated[str, \"The url of a blog post to scrape\"]) -> Annotated[str, \"The scraped content\"]:\n",
    "    \"\"\"\n",
    "    Load a document using the specified loader class and website URL.\n",
    "\n",
    "    Args:\n",
    "    loader_class (class): The class of the loader to be used.\n",
    "    website_url (str): The URL of the website from which to load the document.\n",
    "\n",
    "    Returns:\n",
    "    str: The loaded document.\n",
    "    \"\"\"\n",
    "    loader_class=WebBaseLoader\n",
    "    loader = loader_class([url])\n",
    "    return loader.load()[0].page_content\n",
    "\n",
    "def chunk_text(text: str):\n",
    "    pass\n",
    "    recursive_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=dbutils.secrets.get(scope=\"myscope\", key=\"aoai-deploymentname\"),\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=125,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    recursive_text_splitter_chunks = recursive_text_splitter.split_text(text=text)\n",
    "    return recursive_text_splitter_chunks\n",
    "\n",
    "url = \"https://chinnychukwudozie.com/2024/07/10/enhancing-document-extraction-with-azure-ai-document-intelligence-and-langchain-for-rag-workflows/\"\n",
    "blog_content = scrape_blog(url=url)\n",
    "print(f\"Character count: {len(blog_content)}\")\n",
    "print(f\"---\\nScraped blog text:\\n---\")\n",
    "print(f\" {blog_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377fd825-99fa-4731-bf50-e622e09e0ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 3: Create the agents and register the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84ec275-6ac3-4214-b2ec-0a84a0e27358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_termination_msg_def(x):\n",
    "    content = x.get(\"content\", \"\")\n",
    "    if content is not None and \"terminate\" in content.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "is_termination_msg_def = is_termination_msg_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4523ebcc-e7fd-4218-bfd5-6368c3c8a42f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create web scraper agent\n",
    "scraper_agent = ConversableAgent(\n",
    "    name=\"WebScraper\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a web scrapper and you can scrape any web page using the tools provided.\"\n",
    "    \"Returns 'TERMINATE', when the scraping is done\",\n",
    ")\n",
    "\n",
    "# Create user proxy agent\n",
    "user_proxy_agent = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=\"Please continue if not finished, otherwise return 'TERMINATE'.\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") is not None\n",
    "    and \"terminate\" in x[\"content\"].lower(),\n",
    ")\n",
    "\n",
    "# Register the function with the agents\n",
    "register_function(\n",
    "    f=scrape_blog,\n",
    "    caller=scraper_agent,\n",
    "    executor=user_proxy_agent,\n",
    "    name=\"scrape_blog\",\n",
    "    description=\"Scrape a blog post and return the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f1be97-ea29-4ddf-a91a-3402ee3b0cd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 4: Start the conversation for scraping web data. We used the reflection_with_llm option for summary method to perform the formatting of the output into a desired format. The summary method is called after the conversation is completed given the complete history of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d862dc-eb0e-4c5d-ac5c-5b4a18214eb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_result = user_proxy_agent.initiate_chat(\n",
    "    recipient=scraper_agent,\n",
    "    message=\"Can you scrape chinnychukwudozie.com/2024/07/10/enhancing-document-extraction-with-azure-ai-document-intelligence-and-langchain-for-rag-workflows for me?\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    "    summary_args={\n",
    "        \"summary_prompt\": \"\"\"\n",
    "                Summarize the scraped content and format summary EXACTLY as follows:\n",
    "---\n",
    "*Company name*:\n",
    "`Acme Corp`\n",
    "---\n",
    "*Website*:\n",
    "`acmecorp.com`\n",
    "---\n",
    "*Description*:\n",
    "`Company that does things.`\n",
    "---\n",
    "*Tags*:\n",
    "`Manufacturing. Retail. E-commerce.`\n",
    "---\n",
    "*Takeaways*:\n",
    "`Provides shareholders with value by selling products.`\n",
    "---\n",
    "*Questions*:\n",
    "`What products or services do they offer? If they make money, how? What is their market share?`\n",
    "---                                                                                                                                          \"\"\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bccb87e2-7633-4f18-8a03-ba46d658cdfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(chat_result.summary)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "blog-scraper-agent",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
