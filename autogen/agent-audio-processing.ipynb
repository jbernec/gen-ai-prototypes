{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4ff674-96ad-4e6a-98b3-7aa325ed54cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### In this notebook, I show an example of how to use the openai whisper and GPT-4o models with Autogen AssistantAgent and UserProxyAgent to recognize and translate audio file that contains a clip from a PodCast.\n",
    "#### The source language is English and Target language is Chinese. The transcribe_text_from_audio function does the transcription while the translate_text function is called as part of a function calling example to execute the translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60cb78b-dcd8-4f6b-807a-d2d812f77415",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1: Import required libraries and define variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "462cc40d-d6d7-4f08-8ccb-f06f9492919f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing_extensions import List, Annotated\n",
    "import autogen\n",
    "import os\n",
    "import whisper\n",
    "from openai import AzureOpenAI\n",
    "from autogen import AssistantAgent, UserProxyAgent, register_function\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "keyVaultName = os.environ[\"KEY_VAULT_NAME\"]\n",
    "KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "\n",
    "source_language = \"English\"\n",
    "target_language = \"Chinese\"\n",
    "azure_openai_endpoint=client.get_secret(name=\"aoai-endpoint\").value\n",
    "azure_openai_api_key=client.get_secret(name=\"aoai-api-key\").value\n",
    "azure_openai_deploymentname = client.get_secret(name=\"aoai-deploymentname\").value\n",
    "\n",
    "video_file = \"C:\\\\source\\\\github\\\\whisper-transcription\\\\peppa pig video clip.mp4\"\n",
    "audio_file = \"C:\\\\source\\\\github\\\\whisper-transcription\\\\output_audio.wav\"\n",
    "podcast_filepath = \"C:\\\\source\\\\github\\\\whisper-transcription\\\\PodcastSnippet.mp3\"\n",
    "source_language = \"English\"\n",
    "target_language = \"Chinese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94056173-2419-4217-8bea-608f1865606c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 2: Define and configure the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a20d644a-b89e-41bb-8aff-3455d314f9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": client.get_secret(name=\"aoai-deploymentname\").value,\n",
    "            \"api_key\": client.get_secret(name=\"aoai-api-key\").value,\n",
    "            \"base_url\": client.get_secret(name=\"aoai-endpoint\").value,\n",
    "            \"api_type\": \"azure\",\n",
    "            \"api_version\": \"2024-02-15-preview\",\n",
    "        }\n",
    "    ],\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "\n",
    "assistant = AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    is_termination_msg=lambda x: \"terminate\" in x.get(\"content\", \"\").lower()\n",
    "    if x.get(\"content\", \"\") is not None\n",
    "    else False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d2f5aa-2d6d-4a43-8513-848cbd848031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transcribe_text_from_audio(\n",
    "    filepath: Annotated[str, \"path of the audio file\"]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This function uses the python pydub package to convert the audio file to wav format\n",
    "      and transcribe the audio file to English text using the openai whisper model\"\"\"\n",
    "    try:\n",
    "        # Extract  audio from the podcast file\n",
    "        audio = AudioSegment.from_mp3(file=filepath)\n",
    "        out_file = \"audio.wav\"\n",
    "        audio.export(out_file, format=\"wav\")\n",
    "        # Load model\n",
    "        model = whisper.load_model(\"small\")\n",
    "        result = model.transcribe(audio=out_file, verbose=False)\n",
    "        transcript = result[\"text\"]\n",
    "        # print(transcript)\n",
    "        return transcript\n",
    "    except FileNotFoundError:\n",
    "        print(\"The specified audio file could not be found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12293a1a-610a-450b-9192-a59c18d8ad49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# one way of registering functions is to use the register_for_llm and register_for_execution decorators or use the register_function method.\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@assistant.register_for_llm(\n",
    "    description=\"using translate_text function to translate the script\"\n",
    ")\n",
    "def translate_text(\n",
    "    source_language: Annotated[str, \"source language\"],\n",
    "    target_language: Annotated[str, \"target language\"],\n",
    ") -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is used for the agent function calling. It is registered with the agents and initiates the \n",
    "    translation operation with the LLM.\"\"\"\n",
    "    \n",
    "    client_aoai = AzureOpenAI(\n",
    "        api_key=azure_openai_api_key,\n",
    "        azure_endpoint=azure_openai_endpoint,\n",
    "        api_version=\"2024-02-15-preview\",\n",
    "    )\n",
    "    input_text = transcribe_text_from_audio(filepath=podcast_filepath).strip()\n",
    "    response = client_aoai.chat.completions.create(\n",
    "        model=azure_openai_deploymentname,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Directly translate the following {source_language} text to a pure {target_language}\"\n",
    "                f\"audio text without additional explanation.: '{input_text}'\",\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=1500,\n",
    "    )\n",
    "    # Correctly access the response content\n",
    "    translated_text = response.choices[0].message.content if response.choices else None\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77d7822c-86d6-47bd-8d55-bc7b51905e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Start the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7cece7e-47a7-4809-9723-9d7094704c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agent_result = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=f\"For the provided podcast file in {audio_file}, recognize the speech and transfer it into a script file, \"\n",
    "    f\"then translate from {source_language} text to a {target_language} text. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b3c62fc-a1e9-4285-920c-930282817180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(agent_result.summary)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "agent-audio-processing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
